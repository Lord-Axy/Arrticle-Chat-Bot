{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RnoadQKy86Oy"
      },
      "outputs": [],
      "source": [
        "%rm -rf LLaMA-Factory\n",
        "!git clone https://github.com/hiyouga/LLaMA-Factory.git\n",
        "%cd LLaMA-Factory\n",
        "%ls\n",
        "!pip install ."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llmtuner import run_exp, ChatModel, export_model\n",
        "\n",
        "\n",
        "import torch\n",
        "torch.cuda.is_available()"
      ],
      "metadata": {
        "id": "xtnXUNtB88p5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are using Mistral-7b-Instruct V2 AI, Feel free to explore other Mistral models at: https://huggingface.co/mistralai Also you can explore other supported architectures in LLaMA Factory official repo\n",
        "\n",
        "Here we are using identity,alpaca_gpt4_en,alpaca_gpt4_zh datasets to fine-tune the model, you can explore other datasets in hugging face according to the need."
      ],
      "metadata": {
        "id": "qoJOXppi9CcL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Here we train the model and export it to a directory called \"E2E_Mistral-7B-ChatBot\"\n",
        "run_exp(dict(\n",
        "  stage=\"sft\",\n",
        "  do_train=True,\n",
        "  model_name_or_path=\"mistralai/Mistral-7B-Instruct-v0.2\",\n",
        "  dataset=\"identity,alpaca_gpt4_en,alpaca_gpt4_zh\",\n",
        "  template=\"mistral\",\n",
        "  finetuning_type=\"lora\",\n",
        "  lora_target=\"all\",\n",
        "  output_dir=\"E2E_Mistral-7B-ChatBot\",\n",
        "  per_device_train_batch_size=4,\n",
        "  gradient_accumulation_steps=4,\n",
        "  lr_scheduler_type=\"cosine\",\n",
        "  logging_steps=10,\n",
        "  save_steps=100,\n",
        "  learning_rate=1e-4,\n",
        "  num_train_epochs=5.0,\n",
        "  max_samples=500,\n",
        "  max_grad_norm=1.0,\n",
        "  fp16=True,\n",
        "))"
      ],
      "metadata": {
        "id": "nWVXFA7l8_dP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that we have a folder created inside the LLaMA-Factory with the trained model checkpoints. We can export the fine tuned model to \"E2E-ChatBot\" directory using the below code"
      ],
      "metadata": {
        "id": "kGEOztw89J_N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "export_model(dict(\n",
        "  model_name_or_path=\"mistralai/Mistral-7B-Instruct-v0.2\",\n",
        "  adapter_name_or_path=\"E2E_Mistral-7B-ChatBot\",\n",
        "  finetuning_type=\"lora\",\n",
        "  template=\"mistral\",\n",
        "  export_dir=\"E2E-ChatBot\",\n",
        "))"
      ],
      "metadata": {
        "id": "t_Nchl0M8_Z0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now Let's play with the trained model"
      ],
      "metadata": {
        "id": "HZVeDEZ89NsP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "E2E_Chat_Bot = ChatModel(\n",
        "    dict(\n",
        "  model_name_or_path=\"mistralai/Mistral-7B-Instruct-v0.2\",\n",
        "  adapter_name_or_path=\"E2E_Mistral-7B-ChatBot\",\n",
        "  finetuning_type=\"lora\",\n",
        "  template=\"mistral\",\n",
        "))"
      ],
      "metadata": {
        "id": "rg49wDbL8_XP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages = []\n",
        "while True:\n",
        "  query = input(\"\\nUser: \")\n",
        "  if query.strip() == \"exit\":\n",
        "    break\n",
        "  if query.strip() == \"clear\":\n",
        "    messages = []\n",
        "    continue\n",
        "\n",
        "  messages.append({\"role\": \"user\", \"content\": query})\n",
        "  print(\"E2E_Chat_Bot: \", end=\"\", flush=True)\n",
        "  response = \"\"\n",
        "  for new_text in E2E_Chat_Bot.stream_chat(messages):\n",
        "    response += new_text\n",
        "  for r in response.split('.'):\n",
        "    print(r.strip(), flush=True)\n",
        "  print()\n",
        "  messages.append({\"role\": \"assistant\", \"content\": response})"
      ],
      "metadata": {
        "id": "wTJLPlef8_Uy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}